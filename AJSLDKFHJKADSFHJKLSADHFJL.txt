wzu
42 1
422
423
‘24
25

77777777777 m I‘m-I‘lillb L\'lll.llk.ll\.;ll.‘l‘.lll ‘xwvﬁlkIIHH. HI 1"”
piricnl Methods in Natural Language Processing 1p“.
(Is'MNlJ-"l 2017. ht. t [as : / / n l p . Stanford .

edu/pubs / jiaZO'l "iadversa [1a 1 .pdi‘.

 

1.112. “ Tr”
A. Kurakin. l. Goodi‘ellow, and S. Bengio. 2017. Ad- _, . if;
versarial machine learning at scale. In International ’3" ’r’
Conference on Learning Representations (ICLR).
R. Liu, W. wei. W. Mao, and M Chikina. 2017. Phrase
conductor on mum—layered attention for machine

comprehension. https : / /arxiv . org/pdf/
1710.10504 .pdf.

r-s

P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. 2016. 41,9

Squad: 100,000+ questions for machine compre- 480

hension of text. In Empirical Methods in Natu-

ral Language Processing (EMNLP) 2016. https -.
//arxiv . org/pdf/1606 . 05250 .pdf.

M. Seo, A. Kembhavi, A. Farhadi, and H. Hajishirzi. 483
2017. Bi—directional attention ﬂow for machine
comprehension. In Empirical Methods in Natural £185 “3:. a
Language Processing (EMNLP) 2017. https: €186 .- a;
//arxiv.org/pdf/l6ll.Ol603.pdf. p

1333‘? \\ $6 a “3‘

C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna. D. Er— $38 0 r‘

han, I. Goodfellow, and R. Fergus. 2014. intriguing . .
properties of neural networks. In International Con- 43%

ference on Learning Representations (ICLR). sign

m 1‘;
. A L0 0 M 7
' ' ‘39? 9" 4 C9 ' .
S. Wang and J. Jiang. 2017. Machine comprehension } g?) 9, . y, i- L.
using match-lstm and answer pointer. In I CLR. as“ ,2

3.93
W. Wang, N. Yang, F. Wei, B. Chang, and M. Zhou. .-
2017. Gated self-matching networks for reading {‘93

comprehension and question answering. ln ACL. $35

D. Weissenborn, G. Wiese, and L. Seiffe. 2017. use
Making neural qa as simple as p0ss1ble but not as“?
simpler. https://arxiv.org/pdf/1703. W8
O4816.pdf. mg

W 6..., '3‘“
«ﬁx .

W7“
